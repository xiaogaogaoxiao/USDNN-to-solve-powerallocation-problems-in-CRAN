{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgV0qSY702fT"
   },
   "source": [
    "### Packages  \n",
    "\n",
    "First you need to install all packages which are compatible with the latset version of python. \n",
    "\n",
    "Installed packages need to be upgraded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r requirements (line 1)) (3.3.4)\n",
      "Requirement already satisfied: gekko in /users/yacibena84/.local/lib/python3.8/site-packages (from -r requirements (line 2)) (1.0.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from -r requirements (line 3)) (1.6.0)\n",
      "Requirement already satisfied: seaborn in /users/yacibena84/.local/lib/python3.8/site-packages (from -r requirements (line 4)) (0.11.2)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (from -r requirements (line 5)) (0.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from -r requirements (line 6)) (2.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements (line 1)) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements (line 1)) (8.1.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements (line 1)) (1.20.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /users/yacibena84/.local/lib/python3.8/site-packages (from matplotlib->-r requirements (line 1)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /users/yacibena84/.local/lib/python3.8/site-packages (from matplotlib->-r requirements (line 1)) (2.8.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from cycler>=0.10->matplotlib->-r requirements (line 1)) (1.15.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.8/dist-packages (from seaborn->-r requirements (line 4)) (1.2.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas>=0.23->seaborn->-r requirements (line 4)) (2019.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sklearn->-r requirements (line 5)) (0.24.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (0.2.0)\n",
      "Collecting numpy>=1.15\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (2.4.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (1.12.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (3.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (1.1.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (0.11.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (1.32.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (2.4.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (0.36.2)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements (line 6)) (2.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements (line 6)) (0.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements (line 6)) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements (line 6)) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements (line 6)) (1.25.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements (line 6)) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements (line 6)) (3.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements (line 6)) (45.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements (line 6)) (0.2.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements (line 6)) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements (line 6)) (4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->-r requirements (line 6)) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->-r requirements (line 6)) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements (line 6)) (0.4.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn->-r requirements (line 5)) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn->-r requirements (line 5)) (2.1.0)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "phased-lstm-keras 1.0.2 requires keras>=2.0.2, but you have keras 0.3.1 which is incompatible.\n",
      "keras-vgg-buddy 0.0.5 requires Keras>=0.3.2, but you have keras 0.3.1 which is incompatible.\n",
      "keras-rtst 0.0.9 requires Keras==0.3.3, but you have keras 0.3.1 which is incompatible.\n",
      "keras-rl 0.4.2 requires keras>=2.0.7, but you have keras 0.3.1 which is incompatible.\n",
      "keras-resnet 0.2.0 requires keras>=2.2.4, but you have keras 0.3.1 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.19.5\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5676,
     "status": "ok",
     "timestamp": 1622022141814,
     "user": {
      "displayName": "lacom digital",
      "photoUrl": "",
      "userId": "01338797937786397342"
     },
     "user_tz": -120
    },
    "id": "0-znimLQ2zlc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 14:48:44.451415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import nn\n",
    "\n",
    "\n",
    "py_file_location = \"...\"\n",
    "os.path.abspath(os.path.join(os.path.dirname(py_file_location), os.path.pardir))\n",
    "\n",
    "from model_DL import *\n",
    "from functions import *\n",
    "from metrics import *\n",
    "from DNN_metrics import *\n",
    "from data_generator import *\n",
    "from loss_function import *\n",
    "from optimization import *\n",
    "\n",
    "\n",
    "\n",
    "#tf.device('GPU:1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GPU if it is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 14:48:45.782722: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-10 14:48:45.784170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-10 14:48:45.851967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 44.49GiB deviceMemoryBandwidth: 581.23GiB/s\n",
      "2022-03-10 14:48:45.852461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:5e:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 44.49GiB deviceMemoryBandwidth: 581.23GiB/s\n",
      "2022-03-10 14:48:45.852507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-10 14:48:45.862285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-10 14:48:45.862348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-10 14:48:45.891899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-10 14:48:45.892940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-10 14:48:45.895550: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-10 14:48:45.898040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-10 14:48:45.899237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-10 14:48:45.900061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-03-10 14:48:45.901454: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-10 14:48:45.903992: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-10 14:48:46.059696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 44.49GiB deviceMemoryBandwidth: 581.23GiB/s\n",
      "2022-03-10 14:48:46.059912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:5e:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 44.49GiB deviceMemoryBandwidth: 581.23GiB/s\n",
      "2022-03-10 14:48:46.059939: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-10 14:48:46.059962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-10 14:48:46.059971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-10 14:48:46.059980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-10 14:48:46.059988: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-10 14:48:46.059997: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-10 14:48:46.060006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-10 14:48:46.060014: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-10 14:48:46.060697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-03-10 14:48:46.060728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-10 14:48:46.865642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-10 14:48:46.865678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2022-03-10 14:48:46.865683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2022-03-10 14:48:46.865686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2022-03-10 14:48:46.866726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:lo"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f05cbd7ec00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calhost/replica:0/task:0/device:GPU:0 with 3943 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:3b:00.0, compute capability: 7.5)\n",
      "2022-03-10 14:48:46.867518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 42242 MB memory) -> physical GPU (device: 1, name: Quadro RTX 8000, pci bus id: 0000:5e:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())\n",
    "tf.device('GPU:1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzSvFblBUGab"
   },
   "source": [
    "### Channel gain creation\n",
    "\n",
    "First, you can choose the type of channel to perform the simulations. To do this, simply select the type by entering the number of the desired channel type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        1. Channel gain with gaussian fading [1]\n",
      "        2. Channel gain with Anne model [2]\n",
      "        3. Channel gain with Uniform distribution[3] \n",
      "        4. Channel gain with Rician fading [4]\n",
      "        5. Channel gain with Nakagami fading [5]\n",
      "        6.Exit/Quit\n",
      "        \n",
      "Select channel gain\n",
      "1\n",
      "Channel gain created\n"
     ]
    }
   ],
   "source": [
    "h_PP, h_PR, h_RP, h_SS, h_SR, h_RS, h_SP, h_PS = channel_type() # train data-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        1. Channel gain with gaussian fading [1]\n",
      "        2. Channel gain with Anne model [2]\n",
      "        3. Channel gain with Uniform distribution[3] \n",
      "        4. Channel gain with Rician fading [4]\n",
      "        5. Channel gain with Nakagami fading [5]\n",
      "        6.Exit/Quit\n",
      "        \n",
      "Select channel gain\n",
      "1\n",
      "Channel gain created\n"
     ]
    }
   ],
   "source": [
    "t_h_PP, t_h_PR, t_h_RP, t_h_SS, t_h_SR, t_h_RS, t_h_SP, t_h_PS = channel_type() # test data-type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert channel coefficient to channel gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_PP, g_PR, g_RP, g_SS, g_SR, g_RS, g_SP, g_PS =\\\n",
    "np.power(h_PP, 2), np.power(h_PR, 2), np.power(h_RP, 2), np.power(h_SS, 2)\\\n",
    ", np.power(h_SR, 2), np.power(h_RS, 2), np.power(h_SP, 2), np.power(h_PS, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_g_PP, t_g_PR, t_g_RP, t_g_SS, t_g_SR, t_g_RS, t_g_SP, t_g_PS =\\\n",
    "np.power(t_h_PP, 2), np.power(t_h_PR, 2), np.power(t_h_RP, 2), np.power(t_h_SS, 2)\\\n",
    ", np.power(t_h_SR, 2), np.power(t_h_RS, 2), np.power(t_h_SP, 2), np.power(t_h_PS, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data\n",
    "\n",
    "After data creation, it must be filtred to avoide the division by zero on the custom loss function for that we maintint only channal gain greater then a fixed threshold s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_RP, g_PP, g_SR, g_PR, g_SS, g_RS, g_SP, g_PS = data_filter(g_RP, g_PP, g_SR, g_PR, g_SS, g_RS, g_SP, g_PS)\n",
    "\n",
    "t_g_RP, t_g_PP, t_g_SR, t_g_PR, t_g_SS, t_g_RS, t_g_SP, t_g_PS = data_filter(t_g_RP, t_g_PP, t_g_SR, t_g_PR, t_g_SS, t_g_RS, t_g_SP, t_g_PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation\n",
    "Create the dataset and save it on a specific path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - data size must be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbr_train = int(1E6)\n",
    "\n",
    "Nbr_test = int(2E5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose your directory path  \n",
    "project_sub_path = \"Dataset-GIT\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"\"\n",
    "  \n",
    "# Path\n",
    "try : \n",
    "\n",
    "    path = os.path.join(parent_dir, project_sub_path)\n",
    "    os.mkdir(path)\n",
    "    print(\"Directory '% s' created\" % project_sub_path)\n",
    "except FileExistsError : \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training set generation finished\n"
     ]
    }
   ],
   "source": [
    "np.savez(os.path.join(parent_dir,project_sub_path,'dataset_train.npz'),\n",
    "         g_PP=g_PP,\n",
    "         g_PS=g_PS,\n",
    "         g_PR=g_PR,\n",
    "         g_SP=g_SP,\n",
    "         g_SS=g_SS,\n",
    "         g_SR=g_SR,\n",
    "         g_RP=g_RP,\n",
    "         g_RS=g_RS\n",
    "         ) \n",
    "\n",
    "print(\"\\n training set generation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruteforce\n",
    "The bruteforce method is applied only for the test set to use it as a benchmark with the DNN predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_results = benchmark_generator(np.stack([t_g_RP[:Nbr_test], t_g_PP[:Nbr_test], t_g_SR[:Nbr_test], t_g_PR[:Nbr_test], t_g_SS[:Nbr_test], t_g_RS[:Nbr_test], t_g_SP[:Nbr_test], t_g_PS[:Nbr_test]], axis=1))\n",
    "\n",
    "np.savez(os.path.join(parent_dir,project_sub_path,'dataset_test_3.npz'),\n",
    "         g_PP=t_g_PP[:Nbr_test],\n",
    "         g_PS=t_g_PS[:Nbr_test],\n",
    "         g_PR=t_g_PR[:Nbr_test],\n",
    "         g_SP=t_g_SP[:Nbr_test],\n",
    "         g_SS=t_g_SS[:Nbr_test],\n",
    "         g_SR=t_g_SR[:Nbr_test],\n",
    "         g_RP=t_g_RP[:Nbr_test],\n",
    "         g_RS=t_g_RS[:Nbr_test],\n",
    "         R_S=bf_results[:,8], \n",
    "         Alpha=bf_results[:,9], \n",
    "         p_R=bf_results[:,10], \n",
    "         p_S=bf_results[:,11]\n",
    "         ) \n",
    "\n",
    "print(\"\\n test set generation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL2JOrM0NfVF"
   },
   "source": [
    "### Split dataset\n",
    "load the dataset if it's already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2259,
     "status": "ok",
     "timestamp": 1621968228124,
     "user": {
      "displayName": "lacom digital",
      "photoUrl": "",
      "userId": "01338797937786397342"
     },
     "user_tz": -120
    },
    "id": "WpmQqKYxNWwg"
   },
   "outputs": [],
   "source": [
    "#Nbr_train = int(1E6)\n",
    "\n",
    "#Nbr_test = int(2E5) \n",
    "\n",
    "# # choose your directory path  \n",
    "project_sub_path = \"Dataset-GIT\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"\"\n",
    "\n",
    "\n",
    "### Train ###\n",
    "\n",
    "dataset_train = np.load(os.path.join(parent_dir,project_sub_path,'dataset_train.npz'))\n",
    "\n",
    "g_11_tr = dataset_train['g_PP'][:Nbr_train]\n",
    "g_12_tr = dataset_train['g_PS'][:Nbr_train]\n",
    "g_1R_tr = dataset_train['g_PR'][:Nbr_train]\n",
    "g_21_tr = dataset_train['g_SP'][:Nbr_train]\n",
    "g_22_tr = dataset_train['g_SS'][:Nbr_train]\n",
    "g_2R_tr = dataset_train['g_SR'][:Nbr_train]\n",
    "g_R1_tr = dataset_train['g_RP'][:Nbr_train]\n",
    "g_R2_tr = dataset_train['g_RS'][:Nbr_train]\n",
    "\n",
    "x_train = np.stack([g_R1_tr, g_11_tr, g_2R_tr, g_1R_tr, g_22_tr, g_R2_tr, g_21_tr, g_12_tr], axis=1)\n",
    "\n",
    "\n",
    "### Test ### \n",
    "\n",
    "dataset_test = np.load(os.path.join(parent_dir,project_sub_path,'dataset_test.npz'))\n",
    "\n",
    "\n",
    "g_11_ts = dataset_test['g_PP'][:Nbr_test]\n",
    "g_12_ts = dataset_test['g_PS'][:Nbr_test]\n",
    "g_1R_ts = dataset_test['g_PR'][:Nbr_test]\n",
    "g_21_ts = dataset_test['g_SP'][:Nbr_test]\n",
    "g_22_ts = dataset_test['g_SS'][:Nbr_test]\n",
    "g_2R_ts = dataset_test['g_SR'][:Nbr_test]\n",
    "g_R1_ts = dataset_test['g_RP'][:Nbr_test]\n",
    "g_R2_ts = dataset_test['g_RS'][:Nbr_test]\n",
    "\n",
    "R_S_ts = dataset_test['R_S']\n",
    "Alpha_ts = dataset_test['Alpha']\n",
    "P_R_ts = dataset_test['p_R']\n",
    "P_S_ts = dataset_test['p_S']\n",
    "\n",
    "\n",
    "x_test = np.stack([g_R1_ts, g_11_ts, g_2R_ts, g_1R_ts, g_22_ts, g_R2_ts, g_21_ts, g_12_ts], axis=1)\n",
    "\n",
    "y_test = np.stack([R_S_ts, Alpha_ts, P_R_ts, P_S_ts], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train DNN with $\\lambda \\in [10^{-1},...,10^{2}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1615370,
     "status": "ok",
     "timestamp": 1621954092361,
     "user": {
      "displayName": "lacom digital",
      "photoUrl": "",
      "userId": "01338797937786397342"
     },
     "user_tz": -120
    },
    "id": "yfQaip6vBGgA",
    "outputId": "c37a34a6-a401-4e89-b28d-a97354975db0"
   },
   "outputs": [],
   "source": [
    "tau = 0.25\n",
    "\n",
    "metrics = [Achievable_Rate(tau), QoS_Violation(tau), Primary_Achievable_Rate_Degradation, Primary_ARD_Percentage(tau)] \n",
    "\n",
    "Epochs = 1000 # Epochs number\n",
    "\n",
    "BS = 4096 # Batch_size\n",
    "\n",
    "VS = 0.2 # Validation set\n",
    "\n",
    "LD = {'10_-1':10**-1, '10_-0.75':10**-0.75, '10_-0.5':10**-0.5, '10_-0.25':10**-0.25, '10_0':10**0, '10_0.25':10**0.25, '10_0.5':10**0.5, '10_0.75':10**0.75, '10_1':10**1, '10_1.25':10**1.25, '10_1.5':10**1.5, '10_1.75':10**1.75, '10_2':10**2}\n",
    "\n",
    "LR = 10**-4 #{'10_-4':10**-4}\n",
    "\n",
    "root_dir ='DNN'\n",
    "\n",
    "for ld_k, ld_v in LD.items():\n",
    "    \n",
    "    #Create a new directory (a folder) in Drive\n",
    "\n",
    "    lambda_dir = root_dir+'/lambda = '+ld_k+'/weights/'\n",
    "  \n",
    "    history_dir = root_dir+'/lambda = '+ld_k+'/history/'\n",
    "\n",
    "    tf.io.gfile.makedirs(lambda_dir)\n",
    "\n",
    "    tf.io.gfile.makedirs(history_dir)\n",
    "\n",
    "    #for lr_k, lr_v in LR.items(): add a loop if browsing learning rate needed\n",
    "\n",
    "    model = get_model_DF(x_train, loss_DF_WN(ld_v,tau), metrics,'sigmoid', custom_sigmoid, custom_sigmoid, LR) #lr_v\n",
    "    history = model.fit(x_train, x_train, epochs=Epochs, batch_size=BS, validation_split = VS)\n",
    "\n",
    "    model.save(lambda_dir+ld_k+'.h5')\n",
    "    np.save(history_dir+ld_k+'.npy',history.history)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
